\section{Модели эмбединга}
\subsection{Word2vec}

Word2vec — это метод в области обработки естественного языка (NLP), предназначенный для получения векторных представлений слов. Эти векторы отражают смысл слова, основываясь на его окружении в тексте. Алгоритм Word2vec оценивает такие представления, моделируя текст из большого корпуса. После обучения модель может находить синонимичные слова или предлагать дополнения к частично введённым предложениям. Word2vec был разработан Томашем Миколовым, Каем Ченом, Грегом Коррадо, Ильёй Суцкевером и Джеффом Дином в Google и опубликован в 2013 году .

Word2vec представляет слово в виде вектора высокой размерности, который отражает отношения между словами. В частности, слова, встречающиеся в похожих контекстах, отображаются в близкие друг к другу векторы по косинусному сходству. Это позволяет оценить семантическую близость между словами: например, векторы для walk и ran, but и however, Berlin и Germany будут находиться рядом в пространстве.

Word2vec — это обобщённое название для группы моделей, предназначенных для создания эмбедингов слов. Эти модели являются мелкими двухслойными нейронными сетями, обучаемыми на восстановление лингвистического контекста слов. На вход Word2vec получает большой текстовый корпус и строит отображение слов в векторное пространство, обычно размерностью в несколько сотен, где каждому уникальному слову присваивается свой вектор.

Word2vec использует одну из двух архитектур для получения распределённых представлений слов:

\subsubsection{CBOW}
CBOW - модель непрерывного мешка слов.

CBOW можно представить как задачу «вставить пропущенное слово», при которой эмбединги слов обучаются таким образом, чтобы предсказать целевое слово на основе окружающих. Семантически похожие слова оказывают схожее влияние на вероятность появления соседних слов. Порядок слов в контексте не учитывается (допущение мешка слов).

Предположим, что каждое слово в корпусе должно предсказываться на основе соседей в окне шириной 4. Тогда множество относительных индексов соседей будет:
\begin{equation}
N = \{-4, -3, -2, -1, +1, +2, +3, +4\}
\end{equation}


Целевая функция обучения:
\begin{equation}
\prod_{i \in C} \Pr(w_i \mid \{w_j : j \in N+i\})
\end{equation}

Для повышения численной устойчивости берём логарифм:
\begin{equation}
\sum_{i \in C} \log \Pr(w_i \mid \{w_j : j \in N+i\})
\end{equation}

В нашем вероятностном представлении сумма векторов контекстных слов задаётся как:
\begin{equation}
v := \sum_{j \in N+i} v_{w_j}
\end{equation}

Тогда вероятность определяется через softmax от скалярного произведения:
\begin{equation}
\Pr(w \mid \{w_j : j \in N+i\}) := \frac{e^{v_w \cdot v}}{\sum_{w' \in V} e^{v_{w'} \cdot v}}
\end{equation}

Упрощённая форма функции потерь (которая подлежит максимизации):

\begin{equation}
\sum_{i \in C, j \in N+i} \left( v_{w_i} \cdot v_{w_j} - \ln \sum_{w \in V} e^{v_w \cdot v_{w_j}} \right)
\end{equation}

В (10) левая часть — вычисляется быстро, но правая — медленно, поскольку требует суммирования по всему словарю для каждой позиции. Это делает градиентный подъём вычислительно сложным, поэтому авторы использовали численные приближения (например, negative sampling).

\subsubsection{Skip-gram}

Skip-gram - непрерывная модель со скользящим окном.
Skip-gram придаёт больший вес близким контекстным словам и меньший —более удалённым. 
 
Идея Skip-gram заключается в том, что вектор слова должен быть близок к векторам его сосед, то есть использовать его для предсказания всех его соседей:

Пусть V— множество всех слов, встречающихся в корпусе C.
Наша цель — обучить один вектор  $W $ для каждого слова ${\displaystyle w\in V}:$
\begin{equation}
\prod_{i \in C} \Pr(\{w_j : j \in N+i\} \mid w_i)
\end{equation}

Так как соседние слова предсказываются независимо:
\begin{equation}
\Pr(\{w_j : j \in N+i\} \mid w_i) = \prod_{j \in N+i} \Pr(w_j \mid w_i)
\end{equation}
Берём логарифм:
\begin{equation}
\sum_{i \in C, j \in N+i} \ln \Pr(w_j \mid w_i)
\end{equation}
Снова применяем softmax:

\begin{equation}
\sum_{i \in C, j \in N+i} \left( v_{w_i} \cdot v_{w_j} - \ln \sum_{w \in V} e^{v_w \cdot v_{w_i}} \right)
\end{equation}
		

\subsubsection{Сравнение CBOW и Skip-gram}


В обеих архитектурах Word2vec обрабатывает как отдельные слова, так и контекстное окно, перемещающееся по корпусу.
CBOW работает быстрее, тогда как Skip-gram лучше справляется с редкими словами






<пусть будет как ссылка>Корпус  —  последовательность слов. И CBOW, и Skip-gram — это методы, позволяющие обучить один вектор для каждого слова, встречающегося в корпусе.


После обучения полученные эмбединги размещаются в векторном пространстве так, что слова, часто встречающиеся в схожих контекстах (то есть схожие по смыслу и грамматике), находятся близко друг к другу. Менее схожие слова расположены дальше друг от друга.


Хотя модель Word2vec показывает высокую эффективность в обучении векторных представлений слов, она обладает рядом ограничений, особенно при работе с более сложными языковыми структурами и контекстно-зависимыми значениями слов. В Word2vec каждому слову сопоставляется один фиксированный вектор, независимо от контекста, в котором это слово используется. Это означает, что омонимы, синонимы и многозначные слова будут иметь одинаковое представление, даже если они используются в совершенно разных ситуациях.

Кроме того, Word2vec обучается на основе локальных контекстов фиксированной длины, что ограничивает его способность улавливать дальние зависимости между словами в предложении. Также модели Word2vec не имеют внутреннего механизма для анализа структуры предложения и порядка слов, что важно для понимания сложных синтаксических и семантических связей.

Для преодоления этих ограничений были разработаны контекстные языковые модели, такие как BERT (Bidirectional Encoder Representations from Transformers). 